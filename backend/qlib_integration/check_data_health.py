from loguru import logger
import os
from typing import Optional, Dict, Any
from pathlib import Path

import fire
import pandas as pd
import qlib
from tqdm import tqdm

from qlib.data import D


class DataHealthChecker:
    """
    æ•°æ®å¥åº·æ£€æŸ¥å™¨ï¼Œç”¨äºæ£€æŸ¥OHLCVæ•°æ®çš„è´¨é‡ï¼Œæ”¯æŒåŠ å¯†è´§å¸æ ¼å¼å’Œå¤šç§é¢‘ç‡
    """

    def __init__(
        self,
        csv_path=None,
        qlib_dir=None,
        freq="day",
        large_step_threshold_price=0.5,
        large_step_threshold_volume=3,
        missing_data_num=0,
        market="all",
    ):
        """
        åˆå§‹åŒ–æ•°æ®å¥åº·æ£€æŸ¥å™¨
        
        Args:
            csv_path: CSVæ–‡ä»¶ç›®å½•è·¯å¾„
            qlib_dir: QLibæ•°æ®ç›®å½•è·¯å¾„
            freq: æ•°æ®é¢‘ç‡ï¼Œå¦‚"day"ã€"1min"ç­‰
            large_step_threshold_price: ä»·æ ¼å¤§å¹…å˜åŠ¨é˜ˆå€¼
            large_step_threshold_volume: æˆäº¤é‡å¤§å¹…å˜åŠ¨é˜ˆå€¼
            missing_data_num: ç¼ºå¤±æ•°æ®é˜ˆå€¼
            market: å¸‚åœºæ ‡è¯†ï¼Œé»˜è®¤ä¸º"all"
        """
        assert csv_path or qlib_dir, "One of csv_path or qlib_dir should be provided."
        assert not (csv_path and qlib_dir), "Only one of csv_path or qlib_dir should be provided."

        self.data = {}
        self.problems = {}
        self.freq = freq
        self.large_step_threshold_price = large_step_threshold_price
        self.large_step_threshold_volume = large_step_threshold_volume
        self.missing_data_num = missing_data_num
        self.market = market

        if csv_path:
            assert os.path.isdir(csv_path), f"{csv_path} should be a directory."
            files = [f for f in os.listdir(csv_path) if f.endswith(".csv")]
            for filename in tqdm(files, desc="Loading data"):
                df = pd.read_csv(os.path.join(csv_path, filename))
                self.data[filename] = df

        elif qlib_dir:
            # é’ˆå¯¹åŠ å¯†è´§å¸æ•°æ®å’Œä¸åŒé¢‘ç‡çš„åˆå§‹åŒ–
            self.qlib_dir = qlib_dir
            self.init_qlib_with_freq()
            self.load_qlib_data()
    
    def init_qlib_with_freq(self):
        """
        æ ¹æ®æŒ‡å®šé¢‘ç‡åˆå§‹åŒ–QLibï¼Œæ”¯æŒåŠ å¯†è´§å¸æ•°æ®æ ¼å¼
        """
        try:
            # æ„å»ºåŒ…å«é¢‘ç‡çš„é…ç½®
            provider_uri_dict = {
                self.freq: os.path.join(self.qlib_dir, self.freq)
            }
            
            # å°è¯•ç›´æ¥ç”¨é¢‘ç‡è·¯å¾„åˆå§‹åŒ–
            qlib.init(provider_uri=provider_uri_dict)
            logger.info(f"âœ… QLib initialized successfully with frequency: {self.freq}")
        except Exception as e:
            logger.warning(f"Failed to initialize with frequency-specific path: {e}")
            logger.info("Trying default initialization...")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨é»˜è®¤è·¯å¾„åˆå§‹åŒ–
            qlib.init(provider_uri=self.qlib_dir)
    
    def load_qlib_data(self):
        """
        åŠ è½½QLibæ ¼å¼çš„æ•°æ®ï¼Œæ”¯æŒåŠ å¯†è´§å¸å’Œä¸åŒé¢‘ç‡
        """
        try:
            # å°è¯•è·å–æ‰€æœ‰å¯ç”¨çš„instruments
            instruments = D.instruments(market=self.market)
            instrument_list = D.list_instruments(instruments=instruments, as_list=True, freq=self.freq)
            
            if not instrument_list:
                logger.warning(f"No instruments found for frequency: {self.freq}")
                # å°è¯•ç›´æ¥ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–
                self._load_instruments_from_filesystem()
                return
            
            # å®šä¹‰éœ€è¦çš„å­—æ®µï¼ŒåŠ å¯†è´§å¸å¯èƒ½ä¸éœ€è¦factor
            required_fields = ["$open", "$close", "$low", "$high", "$volume"]
            # å¯¹äºæ—¥é¢‘æ•°æ®ï¼Œå°è¯•åŒ…å«factorå­—æ®µ
            if self.freq in ["day", "1d"]:
                required_fields.append("$factor")
            
            logger.info(f"Loading {len(instrument_list)} instruments with frequency: {self.freq}")
            
            for instrument in tqdm(instrument_list, desc="Loading QLib data"):
                try:
                    df = D.features([instrument], required_fields, freq=self.freq)
                    # é‡å‘½ååˆ—
                    rename_map = {
                        "$open": "open",
                        "$close": "close",
                        "$low": "low",
                        "$high": "high",
                        "$volume": "volume"
                    }
                    if "$factor" in df.columns:
                        rename_map["$factor"] = "factor"
                    
                    df.rename(columns=rename_map, inplace=True)
                    self.data[instrument] = df
                except Exception as e:
                    logger.error(f"Failed to load instrument {instrument}: {e}")
            
            logger.info(f"Successfully loaded {len(self.data)} instruments")
            
        except Exception as e:
            logger.error(f"Error loading QLib data: {e}")
            # å¦‚æœQLib APIå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–
            self._load_instruments_from_filesystem()
    
    def _load_instruments_from_filesystem(self):
        """
        ä»æ–‡ä»¶ç³»ç»Ÿç›´æ¥è¯»å–æ•°æ®ï¼Œä½œä¸ºQLib APIçš„å¤‡é€‰æ–¹æ¡ˆ
        """
        logger.info("Attempting to load data directly from filesystem...")
        
        # å°è¯•æŸ¥æ‰¾instrumentsç›®å½•
        instruments_dir = os.path.join(self.qlib_dir, "instruments")
        if os.path.exists(instruments_dir):
            # è¯»å–instrumentsæ–‡ä»¶
            for root, _, files in os.walk(instruments_dir):
                for file in files:
                    if file.endswith(".txt"):
                        market_name = os.path.splitext(file)[0]
                        instrument_file = os.path.join(root, file)
                        try:
                            with open(instrument_file, 'r') as f:
                                instruments = []
                                for line in f:
                                    stripped_line = line.strip()
                                    if stripped_line:
                                        # å¤„ç†ç©ºæ ¼åˆ†éš”çš„æ ¼å¼ï¼Œæå–ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºinstrumentåç§°
                                        parts = stripped_line.split()
                                        if parts:
                                            instrument = parts[0]
                                            instruments.append(instrument)
                            
                            logger.info(f"Found {len(instruments)} instruments in {market_name}")
                            
                            # å°è¯•åŠ è½½æ¯ä¸ªinstrumentçš„æ•°æ®
                            for instrument in instruments:
                                self._load_instrument_data(instrument)
                        except Exception as e:
                            logger.error(f"Failed to process instruments file {instrument_file}: {e}")
        else:
            logger.warning(f"Instruments directory not found: {instruments_dir}")
            # å°è¯•ç›´æ¥æŸ¥æ‰¾featuresç›®å½•ä¸‹çš„æ–‡ä»¶
            features_dir = os.path.join(self.qlib_dir, "features")
            if os.path.exists(features_dir):
                logger.info("Looking for features data...")
                # è¿™é‡Œå¯ä»¥æ·»åŠ ç›´æ¥è¯»å–featuresæ•°æ®çš„é€»è¾‘
            else:
                logger.warning(f"Features directory not found: {features_dir}")
    
    def _load_instrument_data(self, instrument):
        """
        å°è¯•åŠ è½½å•ä¸ªinstrumentçš„æ•°æ®
        
        Args:
            instrument: äº¤æ˜“å¯¹/è‚¡ç¥¨ä»£ç 
        """
        try:
            # åŠ å¯†è´§å¸æ•°æ®é€šå¸¸å­˜å‚¨åœ¨featuresç›®å½•ä¸‹çš„ç‰¹å®šé¢‘ç‡æ–‡ä»¶å¤¹ä¸­
            # æ„å»ºå¯èƒ½çš„æ•°æ®æ–‡ä»¶è·¯å¾„
            features_dir = os.path.join(self.qlib_dir, "features", self.freq)
            instrument_data_path = os.path.join(features_dir, f"{instrument}.bin")
            
            # å°è¯•ä½¿ç”¨QLibçš„D.featuresç›´æ¥åŠ è½½ç‰¹å®šinstrument
            if instrument not in self.data:
                try:
                    # ä¸ºå•ä¸ªinstrumentå®šä¹‰éœ€è¦çš„å­—æ®µ
                    required_fields = ["$open", "$close", "$low", "$high", "$volume"]
                    if self.freq in ["day", "1d"]:
                        required_fields.append("$factor")
                    
                    logger.debug(f"Trying to load {instrument} using D.features")
                    df = D.features([instrument], required_fields, freq=self.freq)
                    
                    # é‡å‘½ååˆ—
                    rename_map = {
                        "$open": "open",
                        "$close": "close",
                        "$low": "low",
                        "$high": "high",
                        "$volume": "volume"
                    }
                    if "$factor" in df.columns:
                        rename_map["$factor"] = "factor"
                    
                    df.rename(columns=rename_map, inplace=True)
                    self.data[instrument] = df
                    logger.info(f"Successfully loaded data for {instrument}")
                except Exception as e:
                    logger.warning(f"Failed to load {instrument} using D.features: {e}")
                    
                    # å¦‚æœD.featureså¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾å¯èƒ½çš„CSVæˆ–å…¶ä»–æ ¼å¼æ–‡ä»¶
                    # æ£€æŸ¥å¸¸è§çš„æ•°æ®æ–‡ä»¶è·¯å¾„
                    potential_paths = [
                        os.path.join(self.qlib_dir, self.freq, f"{instrument}.csv"),
                        os.path.join(features_dir, f"{instrument}.csv"),
                        os.path.join(self.qlib_dir, "features", f"{instrument}_{self.freq}.csv")
                    ]
                    
                    for path in potential_paths:
                        if os.path.exists(path):
                            logger.info(f"Found data file at {path}")
                            try:
                                df = pd.read_csv(path)
                                # å°è¯•è§£ææ—¥æœŸç´¢å¼•
                                if 'datetime' in df.columns:
                                    df.set_index('datetime', inplace=True)
                                elif 'date' in df.columns:
                                    df.set_index('date', inplace=True)
                                self.data[instrument] = df
                                logger.info(f"Loaded {instrument} data from {path}")
                                break
                            except Exception as csv_e:
                                logger.error(f"Failed to read {path}: {csv_e}")
        except Exception as e:
            logger.error(f"Failed to load data for {instrument}: {e}")

    def check_missing_data(self) -> Optional[pd.DataFrame]:
        """
        æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦å­˜åœ¨ç¼ºå¤±å€¼
        
        Returns:
            åŒ…å«ç¼ºå¤±æ•°æ®ä¿¡æ¯çš„DataFrameï¼Œå¦‚æœæ²¡æœ‰ç¼ºå¤±åˆ™è¿”å›None
        """
        result_dict = {
            "instruments": [],
            "open": [],
            "high": [],
            "low": [],
            "close": [],
            "volume": [],
        }
        
        for filename, df in self.data.items():
            try:
                # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
                required_columns = ["open", "high", "low", "close", "volume"]
                for col in required_columns:
                    if col not in df.columns:
                        df[col] = pd.NA
                
                missing_data_columns = df.isnull().sum()[df.isnull().sum() > self.missing_data_num].index.tolist()
                if len(missing_data_columns) > 0:
                    result_dict["instruments"].append(filename)
                    for col in required_columns:
                        result_dict[col].append(df.isnull().sum()[col] if col in df.columns else len(df))
            except Exception as e:
                logger.error(f"Error checking missing data for {filename}: {e}")

        result_df = pd.DataFrame(result_dict).set_index("instruments") if result_dict["instruments"] else None
        if result_df is not None and not result_df.empty:
            return result_df
        else:
            logger.info(f"âœ… There are no missing data.")
            return None

    def check_large_step_changes(self) -> Optional[pd.DataFrame]:
        """
        æ£€æŸ¥OHLCVåˆ—ä¸­æ˜¯å¦å­˜åœ¨è¶…è¿‡é˜ˆå€¼çš„å¤§å¹…å˜åŠ¨
        
        Returns:
            åŒ…å«å¤§å¹…å˜åŠ¨ä¿¡æ¯çš„DataFrameï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
        """
        result_dict = {
            "instruments": [],
            "col_name": [],
            "timestamp": [],
            "pct_change": [],
        }
        
        for filename, df in self.data.items():
            try:
                for col in ["open", "high", "low", "close", "volume"]:
                    if col in df.columns and len(df) > 1:
                        try:
                            # è®¡ç®—ç™¾åˆ†æ¯”å˜åŒ–ï¼Œå¿½ç•¥NaNå€¼
                            pct_change = df[col].pct_change(fill_method=None).abs()
                            threshold = self.large_step_threshold_volume if col == "volume" else self.large_step_threshold_price
                            
                            if pct_change.max() > threshold:
                                large_steps = pct_change[pct_change > threshold]
                                if not large_steps.empty:
                                    # è·å–ç¬¬ä¸€ä¸ªå¤§å¹…å˜åŠ¨çš„æ•°æ®ç‚¹
                                    first_large_step_idx = large_steps.index[0]
                                    
                                    # å¤„ç†ä¸åŒç±»å‹çš„ç´¢å¼•æ ¼å¼
                                    if isinstance(first_large_step_idx, tuple):
                                        # å¯¹äºå¤šçº§ç´¢å¼•
                                        timestamp = str(first_large_step_idx[1])
                                    else:
                                        # å¯¹äºå•çº§ç´¢å¼•
                                        timestamp = str(first_large_step_idx)
                                    
                                    result_dict["instruments"].append(filename)
                                    result_dict["col_name"].append(col)
                                    result_dict["timestamp"].append(timestamp)
                                    result_dict["pct_change"].append(float(pct_change.max()))
                        except Exception as e:
                            logger.error(f"Error calculating pct_change for {filename}:{col}: {e}")
            except Exception as e:
                logger.error(f"Error checking large step changes for {filename}: {e}")

        result_df = pd.DataFrame(result_dict).set_index("instruments") if result_dict["instruments"] else None
        if result_df is not None and not result_df.empty:
            return result_df
        else:
            logger.info(f"âœ… There are no large step changes in the OHLCV column above the threshold.")
            return None

    def check_required_columns(self) -> Optional[pd.DataFrame]:
        """
        æ£€æŸ¥å¿…éœ€çš„OHLCVåˆ—æ˜¯å¦å­˜åœ¨
        
        Returns:
            åŒ…å«ç¼ºå¤±åˆ—ä¿¡æ¯çš„DataFrameï¼Œå¦‚æœæ²¡æœ‰ç¼ºå¤±åˆ™è¿”å›None
        """
        required_columns = ["open", "high", "low", "close", "volume"]
        result_dict = {
            "instruments": [],
            "missing_col": [],
        }
        
        for filename, df in self.data.items():
            try:
                if not all(column in df.columns for column in required_columns):
                    missing_required_columns = [column for column in required_columns if column not in df.columns]
                    result_dict["instruments"].append(filename)
                    result_dict["missing_col"].append(", ".join(missing_required_columns))
            except Exception as e:
                logger.error(f"Error checking required columns for {filename}: {e}")

        result_df = pd.DataFrame(result_dict).set_index("instruments") if result_dict["instruments"] else None
        if result_df is not None and not result_df.empty:
            return result_df
        else:
            logger.info(f"âœ… The columns (OLHCV) are complete and not missing.")
            return None

    def check_missing_factor(self) -> Optional[pd.DataFrame]:
        """
        æ£€æŸ¥factoråˆ—æ˜¯å¦ç¼ºå¤±ï¼ˆé’ˆå¯¹åŠ å¯†è´§å¸æ•°æ®ï¼Œfactoråˆ—æ˜¯å¯é€‰çš„ï¼‰
        
        Returns:
            åŒ…å«factorç¼ºå¤±ä¿¡æ¯çš„DataFrameï¼Œå¦‚æœæ²¡æœ‰é—®é¢˜åˆ™è¿”å›None
        """
        # å¯¹äºåŠ å¯†è´§å¸æ•°æ®ï¼Œfactoråˆ—å¯èƒ½ä¸æ˜¯å¿…éœ€çš„
        if self.freq in ["1min", "5min", "15min", "30min", "60min"]:
            logger.info(f"âš ï¸ Factor column check skipped for {self.freq} frequency data")
            return None
            
        result_dict = {
            "instruments": [],
            "missing_factor_col": [],
            "missing_factor_data": [],
        }
        
        for filename, df in self.data.items():
            try:
                # è·³è¿‡ç‰¹å®šçš„æŒ‡æ•°æ–‡ä»¶æ£€æŸ¥
                if any(idx in str(filename) for idx in ["000300", "000903", "000905"]):
                    continue
                
                has_factor_col = "factor" in df.columns
                result_dict["instruments"].append(filename)
                result_dict["missing_factor_col"].append(not has_factor_col)
                
                if has_factor_col:
                    result_dict["missing_factor_data"].append(df["factor"].isnull().all())
                else:
                    result_dict["missing_factor_data"].append(True)
            except Exception as e:
                logger.error(f"Error checking factor column for {filename}: {e}")

        result_df = pd.DataFrame(result_dict).set_index("instruments") if result_dict["instruments"] else None
        if result_df is not None and not result_df.empty:
            # åªè¿”å›ç¡®å®æœ‰é—®é¢˜çš„è¡Œ
            problematic_rows = result_df[(result_df["missing_factor_col"] == True) | (result_df["missing_factor_data"] == True)]
            if not problematic_rows.empty:
                return problematic_rows
        
        logger.info(f"âœ… The `factor` column check passed.")
        return None

    def check_data(self):
        """
        æ‰§è¡Œæ‰€æœ‰æ•°æ®å¥åº·æ£€æŸ¥å¹¶æ˜¾ç¤ºç»“æœ
        """
        logger.info(f"Starting data health check for {len(self.data)} instruments with frequency: {self.freq}")
        
        check_missing_data_result = self.check_missing_data()
        check_large_step_changes_result = self.check_large_step_changes()
        check_required_columns_result = self.check_required_columns()
        check_missing_factor_result = self.check_missing_factor()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•é—®é¢˜
        has_problems = (
            check_missing_data_result is not None
            or check_large_step_changes_result is not None
            or check_required_columns_result is not None
            or check_missing_factor_result is not None
        )
        
        if has_problems:
            print(f"\nSummary of data health check ({len(self.data)} instruments checked):")
            print("-------------------------------------------------")
            if isinstance(check_missing_data_result, pd.DataFrame):
                logger.warning(f"âŒ There is missing data.")
                print(check_missing_data_result)
                print()
            if isinstance(check_large_step_changes_result, pd.DataFrame):
                logger.warning(f"âŒ The OHLCV column has large step changes.")
                print(check_large_step_changes_result)
                print()
            if isinstance(check_required_columns_result, pd.DataFrame):
                logger.warning(f"âŒ Columns (OLHCV) are missing.")
                print(check_required_columns_result)
                print()
            if isinstance(check_missing_factor_result, pd.DataFrame):
                logger.warning(f"âŒ The factor column does not exist or is empty")
                print(check_missing_factor_result)
                print()
        else:
            logger.info(f"ğŸ‰ All data health checks passed for {len(self.data)} instruments!")
            print(f"\nData health check summary ({len(self.data)} instruments checked):")
            print("-------------------------------------------------")
            print("âœ… No data issues found!")


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logger.remove()
    logger.add(lambda msg: print(msg, end=""), level="INFO")
    fire.Fire(DataHealthChecker)
